{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# whole-brain timeseries extraction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### import rois"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import itertools\n",
    "import re\n",
    "\n",
    "def rois(path):\n",
    "    \"\"\"\n",
    "    import roi centers from (fiji Unzipped RoiSet folder) one frame/zplane in timeseries\n",
    "    Create a mask around this pixel to use as roimask(x,y)\n",
    "    \"\"\"\n",
    "    roifiles = list(filter(lambda f:f.endswith('.roi'), os.listdir(path)))\n",
    "    roi,roimask,roimask_sorted = [{} for i in range(3)]\n",
    "    for n in roifiles:\n",
    "        roi[n]=n.split('.roi')[0]\n",
    "        roi[n]=re.split('-0*',roi[n])\n",
    "        roimask[np.int(roi[n][0])-1,n] = list(itertools.product(list(range(np.int(roi[n][2])-3,np.int(roi[n][2])+3)), list(range(np.int(roi[n][1])-3,np.int(roi[n][1])+3))))\n",
    "        #correct for offset by 1 from numZ: np.int(roi[n][0])-1\n",
    "    n_sorted = sorted(roimask.keys())\n",
    "    for k in n_sorted:\n",
    "        roimask_sorted[k] = roimask[k]\n",
    "    return roimask_sorted"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### sort tiff filenames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "sort filenames\n",
    "\"\"\"\n",
    "def sortfn(path):\n",
    "    listfiles = list(filter(lambda f:f.endswith('ome.tif'), os.listdir(path)))\n",
    "    fnum = {}\n",
    "    for s in listfiles:\n",
    "        s = os.path.join(path,s)\n",
    "        fnum[s]=s.split('.ome.tif')[0]\n",
    "        fnum[s]=fnum[s].split('Default')[1]\n",
    "        if fnum[s] == '':\n",
    "            fnum[s]=\"00\"\n",
    "        if len(fnum[s])==2:\n",
    "            fnum[s]=\"0\"+fnum[s][1]\n",
    "        if len(fnum[s])>=3:\n",
    "            fnum[s]=fnum[s][1:]\n",
    "    fn = sorted(fnum.keys(),key=fnum.get)\n",
    "    return fn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### organize sequential tiffs in callable chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "import pdb\n",
    "import json\n",
    "import base64\n",
    "\n",
    "import matplotlib\n",
    "# matplotlib.use(\"tkAgg\")\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def chunk_ix(shape=None, dims=None, req=None):\n",
    "    \"\"\"indices for a chunk request (expanding a chunk request)\n",
    "\n",
    "    'shape' and 'dims' describe the shape and dims names of an array A of\n",
    "    arbitrary dimensions. 'req' is a (compact, dictionary) request for a chunk\n",
    "    of A (np.ndarray).\n",
    "\n",
    "    arguments\n",
    "    ------\n",
    "    shape (list): array shape\n",
    "    dims (list or str): array dims ('XYZTC' or ['X','Y','Z','T','C'])\n",
    "    req (dict): chunk request (e.g. {'X':1, 'T':[0,2], 'Y':(0,5)})\n",
    "\n",
    "    req format options:\n",
    "        X=1             interpreted as [1]\n",
    "        X=(0,3)         interpreted as range(0,3)\n",
    "        X=[0,3]         interpreted as itself (a list)\n",
    "        X=slice(0,3)    python slice (e.g. slice(None,None,10) for every 10th)\n",
    "        X=None          (default) all values\n",
    "\n",
    "    returns\n",
    "    ------\n",
    "    ix (list):  a list of lists of index values for each dimension. Using the \n",
    "                output, numpy.ix_(ix) is a valid slice of A. Similarly, \n",
    "                itertools.product(*ix) makes a list of tuples for chunk\n",
    "                elements in A.\n",
    "\n",
    "    TODO: how to handle missing dims in the request? Return all/zero/error?\n",
    "            A: if size is 1, use it, otherwise\n",
    "    \"\"\"\n",
    "\n",
    "    #== check if requested dims DNE in the data\n",
    "    for k in req.keys():\n",
    "        if k not in dims:\n",
    "            raise Exception('requested axis (%s) not in dims (%s)' % (k, str(dims)))\n",
    "\n",
    "    #== a partial request, missing some dims, is made explicit\n",
    "    reqloc = {k:v for k,v in req.items()}\n",
    "    for a in dims:\n",
    "        if a not in req.keys():\n",
    "            reqloc[a] = None\n",
    "            #print('WARNING: request set to None for %s' % a)\n",
    "\n",
    "    ix = []\n",
    "    for size, dim in zip(shape, dims):\n",
    "        r = reqloc[dim]\n",
    "        if isinstance(r, int):\n",
    "            ix.append([r])\n",
    "        elif isinstance(r, tuple):\n",
    "            ix.append(list(range(*r)))\n",
    "        elif isinstance(r, list):\n",
    "            ix.append(r)\n",
    "        elif isinstance(r, slice):\n",
    "            ix.append(range(size)[r])\n",
    "        elif r is None:\n",
    "            ix.append(list(range(size)))\n",
    "        else:\n",
    "            raise Exception('chunk index: (%s) not recognized' %\n",
    "                            (str(reqloc[dim])))\n",
    "    return ix\n",
    "\n",
    "class DataChunk(object):\n",
    "    \"\"\"\n",
    "    multidimensional data and metadata\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, data, **kwargs):\n",
    "\n",
    "        self.data = data\n",
    "        self.dims = kwargs.get('dims', None)\n",
    "\n",
    "        self.meta = kwargs.get('meta', {})\n",
    "\n",
    "        if kwargs.get('axes', None) is not None:\n",
    "            raise Exception('axes no longer accepted, used dims instead')\n",
    "\n",
    "        #== derived\n",
    "        self.dim_len = dict(zip(self.dims,self.data.shape))\n",
    "\n",
    "    @property\n",
    "    def shape(self):\n",
    "        return self.data.shape\n",
    "\n",
    "    @property\n",
    "    def dtype(self):\n",
    "        return self.data.dtype\n",
    "\n",
    "    def subchunk(self, req=None, inplace=False, squeeze=False, verbose=False):\n",
    "        \"\"\"return a subchunk of a data (also a DataChunk)\n",
    "\n",
    "        chunk indexing format options:\n",
    "            X=1         interpreted as index 1\n",
    "            X=(0,3)     interpreted as a range\n",
    "            X=[0,3]     interpreted as a list\n",
    "            X=None      all values\n",
    "        \"\"\"\n",
    "\n",
    "        if inplace:\n",
    "            raise Exception('inplace=True not yet implemented')\n",
    "\n",
    "        #== building the chunk request\n",
    "        req_ix = chunk_ix(shape=self.shape, dims=self.dims, req=req)\n",
    "        ix = np.ix_(*req_ix)\n",
    "\n",
    "        chunk = self.data[ix]\n",
    "        dims = self.dims\n",
    "        md = self.meta.copy()\n",
    "        #md['generation'] = 2\n",
    "\n",
    "        if verbose:\n",
    "            print('-----------')\n",
    "            print('dims   :', self.dims)\n",
    "            print('shape   :', self.shape)\n",
    "            print('request:', req)\n",
    "            print('ix     :', ix)\n",
    "            print('chunk  :', chunk)\n",
    "            print('-----------')\n",
    "\n",
    "        return DataChunk(data=chunk, dims=dims, meta=md)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools\n",
    "import pdb\n",
    "import os\n",
    "\n",
    "import tifffile as tf\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "class TiffMetadataParser(object):\n",
    "    def __init__(self, f=None):\n",
    "        raise Exception('TiffMetadataParser DNE, parsing is handled by the TiffReader')\n",
    "\n",
    "\n",
    "\n",
    "class TiffReader(object):\n",
    "    \"\"\"for pulling frame(s) and chunks out of a tiff hyperstack\n",
    "\n",
    "    TODO: lil-tiff (first N frames to a new file, allows fiji inspection\n",
    "          to determine metadata parameters (dim order, num-Z, starting frame\n",
    "          etc...))\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, f=None):\n",
    "\n",
    "        if isinstance(f, list):\n",
    "            self.files = f\n",
    "        else:\n",
    "            self.files = [f]\n",
    "\n",
    "        self.TF = [tf.TiffFile(f) for f in self.files]\n",
    "        self.build_index()\n",
    "\n",
    "        #self.mmmd = [tiff.micromanager_metadata for tiff in self.TF]\n",
    "        return\n",
    "\n",
    "    def build_index(self):\n",
    "        \"\"\"global frame index for >=1 tiff file(s) and metadata\n",
    "\n",
    "        The dataframe df holds all of the tiff page indexing info. Columns\n",
    "        T, Z, and C are self-explanatory. F is the (input) file index and 'ndx'\n",
    "        is the page index WITHIN the corresponding file. Given, TZC, one can\n",
    "        determine F and ndx.\n",
    "\n",
    "        tzc2fndx is a dictionary. Keys are (T,Z,C) tuples (converted to strings)\n",
    "        and values are (F, ndx) tuples\n",
    "\n",
    "        \"\"\"\n",
    "\n",
    "        # extract some info from zeroth series of zeroth file\n",
    "        series0 = self.TF[0].series[0]\n",
    "        axes = series0.axes\n",
    "        shape = series0.shape\n",
    "        dtype = series0.dtype\n",
    "\n",
    "        if axes[-2:] not in ['XY', 'YX']:\n",
    "            raise Exception('expecting the last two axes to be XY or YX ')\n",
    "\n",
    "        # split axes into tzc and xy\n",
    "        xy_axes = axes[-2:]\n",
    "        xy_shape = shape[-2:]\n",
    "        tzc_axes = axes[:-2]\n",
    "        tzc_shape = shape[:-2]\n",
    "        tzc_ndx = list(itertools.product(*[range(n) for n in tzc_shape]))\n",
    "\n",
    "        # a dataframe to hold all the indexing information\n",
    "        df = pd.DataFrame(data=tzc_ndx, columns=list(tzc_axes))\n",
    "        for col in ['T', 'Z', 'C']:\n",
    "            if col not in df.columns:\n",
    "                df[col] = np.zeros(len(df), dtype=int)\n",
    "\n",
    "        filesinfo = []\n",
    "        fndx = []\n",
    "        ndx = []\n",
    "        for i, ff in enumerate(self.files):\n",
    "            tiff = self.TF[i]\n",
    "            num_pages = len(tiff.pages)\n",
    "            fndx += [i]*num_pages\n",
    "            ndx += list(range(num_pages))\n",
    "            filesinfo.append(dict(f=ff, num_pages=num_pages))\n",
    "            \n",
    "        if len(ndx) != len(df):\n",
    "            print('WARNING: something is wrong')\n",
    "            raise Exception()\n",
    "\n",
    "\n",
    "        df['F'] = fndx\n",
    "        df['ndx'] = ndx\n",
    "        df = df[['T', 'Z', 'C', 'F', 'ndx']]\n",
    "\n",
    "        # given TZC, return [file,frame] indices\n",
    "        tzc2fndx = {str(tuple(x[:3])): (x[3],x[4]) for x in df.values}\n",
    "        #tzc2ndx = {str(tuple(x[:3])): x[3],x[4]) for x in df.values}\n",
    "\n",
    "        meta = dict(\n",
    "            _about=\"tiff metadata\",\n",
    "            files=self.files,\n",
    "            filesinfo=filesinfo,\n",
    "            shape=shape,\n",
    "            axes=axes,\n",
    "            dtype=dtype,\n",
    "            axes_xy=xy_axes,\n",
    "            shape_xy=xy_shape,\n",
    "            axes_tzc=tzc_axes,\n",
    "            shape_tzc=tzc_shape,\n",
    "            df_tzc=df,\n",
    "            tzc2fndx=tzc2fndx\n",
    "            )\n",
    "\n",
    "        self.meta = meta\n",
    "\n",
    "        return\n",
    "\n",
    "\n",
    "    def about(self):\n",
    "        \"\"\"helpful information at a glance\"\"\"\n",
    "        print('#------------------------')\n",
    "        print('# TiffReader metadata')\n",
    "        print('#------------------------')\n",
    "        print('num_files :', len(self.meta['files']))\n",
    "        for i, ff in enumerate(self.meta['filesinfo']):\n",
    "            print('file%3.3i   :' % i, ff['f'])\n",
    "        print('pages/file:', [ff['num_pages'] for ff in self.meta['filesinfo']])\n",
    "        print('dtype     :', self.meta['dtype'])\n",
    "        print('axes      :', self.meta['axes'])\n",
    "        print('shape     :', self.meta['shape'])\n",
    "        print('df_tzc    :')\n",
    "        print(pd.concat([self.meta['df_tzc'].head(), self.meta['df_tzc'].tail()]))\n",
    "        print()\n",
    "        return\n",
    "\n",
    "\n",
    "    def getframe(self, Z=0, T=0, C=0):\n",
    "        \"\"\"returns a (YX) frame given (hyper)stack indices Z,T,C\n",
    "\n",
    "        TODO: turbo request of a frame subregion?\n",
    "        TODO: what other metadata to carry forward?\n",
    "        TODO: return a datachunk?\n",
    "        \"\"\"\n",
    "        #index = self.meta['tzc2ndx'][str(tuple([T, Z, C]))]\n",
    "        #data = self.tiff.pages[index].asarray()\n",
    "\n",
    "        f, index = self.meta['tzc2fndx'][str(tuple([T, Z, C]))]\n",
    "        data = self.TF[f].pages[index].asarray()\n",
    "\n",
    "        axes = self.meta['axes_xy']\n",
    "        shape = self.meta['shape_xy']\n",
    "        dtype = self.meta['dtype']\n",
    "        meta = dict(Z=Z, T=T, C=C, dtype=dtype)\n",
    "\n",
    "        frame = dict(data=data, axes=axes, shape=shape, meta=meta)\n",
    "        return frame\n",
    "\n",
    "    def getchunk(self, req=None):\n",
    "        \"\"\"get multiple frames and assemble a DataChunk\n",
    "\n",
    "        TODO: better naming for chunk requests (compact vs full?)\n",
    "        \"\"\"\n",
    "        dtype = self.meta['dtype']\n",
    "\n",
    "        #== confirm that requested axes exist in the file\n",
    "        for a in req.keys():\n",
    "            if a not in self.meta['axes']:\n",
    "                raise Exception('requested axis (%s) not in the tiff axes (%s)'\n",
    "                % (a, self.meta['axes']))\n",
    "\n",
    "        #===========================================================\n",
    "        #== enumerate TZC combinations (frames) and generate ALL\n",
    "        #== of the getframe requests. Each request is a dictionary\n",
    "        #===========================================================\n",
    "        shape_tzc = self.meta['shape_tzc']\n",
    "        axes_tzc = self.meta['axes_tzc']\n",
    "        req_tzc = {k:req.get(k, None) for k in axes_tzc}\n",
    "        ix_tzc = chunk_ix(shape=shape_tzc, dims=axes_tzc, req=req_tzc)\n",
    "        #== list of tuples\n",
    "        TZC = list(itertools.product(*ix_tzc))\n",
    "\n",
    "        #== the XY request (basically a crop) is done on each TZC frame\n",
    "        shape_xy = self.meta['shape_xy']\n",
    "        axes_xy = self.meta['axes_xy']\n",
    "        req_xy = {k:req.get(k, None) for k in 'XY'}\n",
    "        ix_xy = chunk_ix(shape=shape_xy, dims=axes_xy, req=req_xy)\n",
    "\n",
    "        #== determine the shape of the output chunk\n",
    "        new_shape_tzc = [len(x) for x in ix_tzc]\n",
    "        new_shape_xy = [len(x) for x in ix_xy]\n",
    "\n",
    "        #== build a list of frames, then reshape it\n",
    "        shp = [len(TZC)] + new_shape_xy\n",
    "        data = np.zeros(shp, dtype=dtype)\n",
    "        for i, row in enumerate(TZC):\n",
    "            reqi = dict(zip(axes_tzc, row))\n",
    "            frm = self.getframe(**reqi)\n",
    "            # if no req_xy, do not make a DataChunk\n",
    "            if req_xy['X'] == None and req_xy['Y'] == None:\n",
    "                data[i] = frm['data']\n",
    "            else:\n",
    "                frm['dims'] = frm['axes']\n",
    "                frm.pop('axes')\n",
    "                chk = DataChunk(**frm).subchunk(req=req_xy)\n",
    "                data[i] = chk.data\n",
    "            #print(row, dict(zip(axes_tzc, row)))\n",
    "\n",
    "        #== reshape\n",
    "        shape = new_shape_tzc+new_shape_xy\n",
    "        data = data.reshape(shape)\n",
    "\n",
    "        #== build the DataChunk\n",
    "        axes = axes_tzc + axes_xy\n",
    "        meta = dict(_about='DataChunk from a tiff file',\n",
    "                    req=req)\n",
    "        chunk = DataChunk(data=data, dims=axes, meta=meta)\n",
    "\n",
    "        return chunk\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### read tiffs for the requested chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pdb\n",
    "import tifffile as tf\n",
    "import pandas as pd\n",
    "\n",
    "class TiffReader(object):\n",
    "    \"\"\"\n",
    "    for pulling frame(s) and chunks out of a tiff hyperstack\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, f=None):\n",
    "\n",
    "        if isinstance(f, list):\n",
    "            self.files = f\n",
    "        else:\n",
    "            self.files = [f]\n",
    "\n",
    "        self.TF = [tf.TiffFile(f) for f in self.files]\n",
    "        self.build_index()\n",
    "\n",
    "        #self.mmmd = [tiff.micromanager_metadata for tiff in self.TF]\n",
    "        return\n",
    "\n",
    "    def build_index(self):\n",
    "        \"\"\"global frame index for >=1 tiff file(s) and metadata\n",
    "\n",
    "        The dataframe df holds all of the tiff page indexing info. Columns\n",
    "        T, Z, and C are self-explanatory. F is the (input) file index and 'ndx'\n",
    "        is the page index WITHIN the corresponding file. Given, TZC, one can\n",
    "        determine F and ndx.\n",
    "\n",
    "        tzc2fndx is a dictionary. Keys are (T,Z,C) tuples (converted to strings)\n",
    "        and values are (F, ndx) tuples\n",
    "\n",
    "        \"\"\"\n",
    "\n",
    "        # extract some info from zeroth series of zeroth file\n",
    "        series0 = self.TF[0].series[0]\n",
    "        axes = series0.axes\n",
    "        shape = series0.shape\n",
    "        dtype = series0.dtype\n",
    "\n",
    "        if axes[-2:] not in ['XY', 'YX']:\n",
    "            raise Exception('expecting the last two axes to be XY or YX ')\n",
    "\n",
    "        # split axes into tzc and xy\n",
    "        xy_axes = axes[-2:]\n",
    "        xy_shape = shape[-2:]\n",
    "        tzc_axes = axes[:-2]\n",
    "        tzc_shape = shape[:-2]\n",
    "        tzc_ndx = list(itertools.product(*[range(n) for n in tzc_shape]))\n",
    "\n",
    "        # a dataframe to hold all the indexing information\n",
    "        df = pd.DataFrame(data=tzc_ndx, columns=list(tzc_axes))\n",
    "        for col in ['T', 'Z', 'C']:\n",
    "            if col not in df.columns:\n",
    "                df[col] = np.zeros(len(df), dtype=int)\n",
    "\n",
    "        filesinfo = []\n",
    "        fndx = []\n",
    "        ndx = []\n",
    "        for i, ff in enumerate(self.files):\n",
    "            tiff = self.TF[i]\n",
    "            num_pages = len(tiff.pages)\n",
    "            fndx += [i]*num_pages\n",
    "            ndx += list(range(num_pages))\n",
    "            filesinfo.append(dict(f=ff, num_pages=num_pages))\n",
    "            \n",
    "        if len(ndx) != len(df):\n",
    "            print('WARNING: something is wrong')\n",
    "            raise Exception()\n",
    "\n",
    "        df['F'] = fndx\n",
    "        df['ndx'] = ndx\n",
    "        df = df[['T', 'Z', 'C', 'F', 'ndx']]\n",
    "\n",
    "        # given TZC, return [file,frame] indices\n",
    "        tzc2fndx = {str(tuple(x[:3])): (x[3],x[4]) for x in df.values}\n",
    "\n",
    "        meta = dict(\n",
    "            _about=\"tiff metadata\",\n",
    "            files=self.files,\n",
    "            filesinfo=filesinfo,\n",
    "            shape=shape,\n",
    "            axes=axes,\n",
    "            dtype=dtype,\n",
    "            axes_xy=xy_axes,\n",
    "            shape_xy=xy_shape,\n",
    "            axes_tzc=tzc_axes,\n",
    "            shape_tzc=tzc_shape,\n",
    "            df_tzc=df,\n",
    "            tzc2fndx=tzc2fndx\n",
    "            )\n",
    "\n",
    "        self.meta = meta\n",
    "\n",
    "        return\n",
    "\n",
    "    def getframe(self, Z=0, T=0, C=0):\n",
    "        \"\"\"returns a (YX) frame given (hyper)stack indices Z,T,C\n",
    "\n",
    "        TODO: turbo request of a frame subregion?\n",
    "        TODO: what other metadata to carry forward?\n",
    "        TODO: return a datachunk?\n",
    "        \"\"\"\n",
    "        #index = self.meta['tzc2ndx'][str(tuple([T, Z, C]))]\n",
    "        #data = self.tiff.pages[index].asarray()\n",
    "\n",
    "        f, index = self.meta['tzc2fndx'][str(tuple([T, Z, C]))]\n",
    "        data = self.TF[f].pages[index].asarray()\n",
    "\n",
    "        axes = self.meta['axes_xy']\n",
    "        shape = self.meta['shape_xy']\n",
    "        dtype = self.meta['dtype']\n",
    "        meta = dict(Z=Z, T=T, C=C, dtype=dtype)\n",
    "\n",
    "        frame = dict(data=data, axes=axes, shape=shape, meta=meta)\n",
    "        return frame\n",
    "\n",
    "    def getchunk(self, req=None):\n",
    "        \"\"\"get multiple frames and assemble a DataChunk\n",
    "\n",
    "        TODO: better naming for chunk requests (compact vs full?)\n",
    "        \"\"\"\n",
    "        dtype = self.meta['dtype']\n",
    "\n",
    "        #== confirm that requested axes exist in the file\n",
    "        for a in req.keys():\n",
    "            if a not in self.meta['axes']:\n",
    "                raise Exception('requested axis (%s) not in the tiff axes (%s)'\n",
    "                % (a, self.meta['axes']))\n",
    "\n",
    "        #===========================================================\n",
    "        #== enumerate TZC combinations (frames) and generate ALL\n",
    "        #== of the getframe requests. Each request is a dictionary\n",
    "        #===========================================================\n",
    "        shape_tzc = self.meta['shape_tzc']\n",
    "        axes_tzc = self.meta['axes_tzc']\n",
    "        req_tzc = {k:req.get(k, None) for k in axes_tzc}\n",
    "        ix_tzc = chunk_ix(shape=shape_tzc, dims=axes_tzc, req=req_tzc)\n",
    "        #== list of tuples\n",
    "        TZC = list(itertools.product(*ix_tzc))\n",
    "\n",
    "        #== the XY request (basically a crop) is done on each TZC frame\n",
    "        shape_xy = self.meta['shape_xy']\n",
    "        axes_xy = self.meta['axes_xy']\n",
    "        req_xy = {k:req.get(k, None) for k in 'XY'}\n",
    "        ix_xy = chunk_ix(shape=shape_xy, dims=axes_xy, req=req_xy)\n",
    "\n",
    "        #== determine the shape of the output chunk\n",
    "        new_shape_tzc = [len(x) for x in ix_tzc]\n",
    "        new_shape_xy = [len(x) for x in ix_xy]\n",
    "\n",
    "        #== build a list of frames, then reshape it\n",
    "        shp = [len(TZC)] + new_shape_xy\n",
    "        data = np.zeros(shp, dtype=dtype)\n",
    "        for i, row in enumerate(TZC):\n",
    "            reqi = dict(zip(axes_tzc, row))\n",
    "            frm = self.getframe(**reqi)\n",
    "            # if no req_xy, do not make a DataChunk\n",
    "            if req_xy['X'] == None and req_xy['Y'] == None:\n",
    "                data[i] = frm['data']\n",
    "            else:\n",
    "                frm['dims'] = frm['axes']\n",
    "                frm.pop('axes')\n",
    "                chk = DataChunk(**frm).subchunk(req=req_xy)\n",
    "                data[i] = chk.data\n",
    "            #print(row, dict(zip(axes_tzc, row)))\n",
    "\n",
    "        #== reshape\n",
    "        shape = new_shape_tzc+new_shape_xy\n",
    "        data = data.reshape(shape)\n",
    "\n",
    "        #== build the DataChunk\n",
    "        axes = axes_tzc + axes_xy\n",
    "        meta = dict(_about='DataChunk from a tiff file',\n",
    "                    req=req)\n",
    "        chunk = DataChunk(data=data, dims=axes, meta=meta)\n",
    "\n",
    "        return chunk"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### extract timeseries of imported rois from images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Timeseries(object):\n",
    "    def __init__(self):\n",
    "        self.path = r\"C:\\Users\\heeun\\Documents\\Garrison_lab\\somatag_2019\\20190925w2_nls\" #directory where image files for the recording are\n",
    "        self.wormID = \"20190925_w2_nls\"  \n",
    "        self.fnames = sortfn(self.path)\n",
    "        self.roimasks = rois(self.path +'/RoiSet') #rois are drawn in fiji on frames of first volume and their centers are imported in\n",
    "        self.bck = rois(self.path + '/bckg') #one background roi per each frame in first volume is drawn in the darkest region in frame\n",
    "        self.tiffs = TiffReader(f=self.fnames)\n",
    "        self.numZ = 10 #input an integer for number of z positions in 1 volume; 1 if singleplane\n",
    "        self.exptime = 0.02 #input a float for exposure time in seconds\n",
    "        \n",
    "    def ZorganizedTimepoints(self):\n",
    "        \"\"\"\n",
    "        request z organized timepoints across OME Tiff files\n",
    "        \"\"\"\n",
    "        zTidx,zTchunk,zT0 = ({} for i in range(3))\n",
    "        zTidx = [dict(T=list(range(z,self.tiffs.meta['shape'][0],self.numZ))) for z in range(self.numZ)]\n",
    "        for z in range(self.numZ):\n",
    "            zTchunk[z] = self.tiffs.getchunk(req=zTidx[z]) #this takes the longest amount of time to run..\n",
    "            zT0[z] = self.tiffs.getchunk(req=dict(T=z)) #for verifying correct association of rois with image\n",
    "        self.zTchunk = zTchunk\n",
    "        return zT0\n",
    "\n",
    "    def ZorganizedROIs(self):\n",
    "        \"\"\"\n",
    "        Make a list of x,y coordinates of all rois indexed by z plane for the first volume in time\n",
    "        \"\"\"\n",
    "        roixy,bckxy,roilabels=({} for i in range(3))\n",
    "        roilabels=list(enumerate(self.roimasks.keys()))\n",
    "        for z in range(self.numZ):\n",
    "            roixy[z] = []\n",
    "            bckxy[z] = []\n",
    "            for k,v in self.roimasks.items():\n",
    "                if type(k) in [list,tuple,dict] and z in k:\n",
    "                    roixy[z].append(v)\n",
    "            for k,v in self.bck.items():\n",
    "                if type(k) in [list,tuple,dict] and z in k:\n",
    "                    bckxy[z].append(v)\n",
    "        self.roixy = roixy\n",
    "        self.bckxy = bckxy\n",
    "        self.roilabels = roilabels\n",
    "        rlabels=pd.DataFrame.from_dict(roilabels)\n",
    "        rlabels.to_pickle(self.path+\"/rlabels.pkl\")\n",
    "        return\n",
    "    \n",
    "    def roipixels(self):\n",
    "        \"\"\"\n",
    "        Obtain fluorescence intensity values from corresponding image for each (x,y) coordinate\n",
    "        \"\"\"\n",
    "        roi_pixels,roi_pixels_trcorr,bck_pixels = ({} for i in range(3))\n",
    "        for z in range(self.numZ):\n",
    "            for t in range(len(self.zTchunk[z].data)):\n",
    "                roi_pixels[z,t] = [[self.zTchunk[z].data[t][y,x] for (x,y) in self.roixy[z][r]] for r in range(len(self.roixy[z]))]\n",
    "                bck_pixels[z,t] = [[self.zTchunk[z].data[t][y,x] for (x,y) in self.bckxy[z][r]] for r in range(len(self.bckxy[z]))]\n",
    "        self.roi_pixels = roi_pixels\n",
    "        self.bck_pixels = bck_pixels\n",
    "        return\n",
    "\n",
    "    def dFF(self):\n",
    "        \"\"\"\n",
    "        Calculate and save dFF for each roi with and without background subtraction\n",
    "        \"\"\"\n",
    "        os.mkdir(self.path+\"/Quant\")\n",
    "        bckg,roi,roi_nob,roi_nob_mean,roi_mean_nobsub,roi_dFF,roi_dFF_nobsub,roin_dFF,roin_dFF_nobsub,roi_bck,rn_dFF,rn_dFF_nobsub = ({} for i in range(12))\n",
    "        for z in range(self.numZ):\n",
    "            for t in range(len(self.zTchunk[z].data)):\n",
    "                bckg[z,t] = np.mean(self.bck_pixels[z,t])\n",
    "                for r in range(len(self.roixy[z])):\n",
    "                    roi[(z,r),t] = np.mean(self.roi_pixels[z,t][r])\n",
    "                    roi_nob[(z,r),t] = [roi[(z,r),t] - bckg[z,t]]\n",
    "                    roi_mean_nobsub[(z,r)] = np.mean(self.roi_pixels[z,t][r])\n",
    "        for z in range(self.numZ):\n",
    "            for r in range(len(self.roixy[z])):\n",
    "                roi_nob_mean[(z,r)]=[]\n",
    "                for t in range(len(self.zTchunk[z].data)):\n",
    "                    roi_nob_mean[(z,r)].extend(roi_nob[(z,r),t])\n",
    "        for z in range(self.numZ):\n",
    "            for r in range(len(self.roixy[z])):\n",
    "                roin_dFF[(z,r)]=[]\n",
    "                roin_dFF_nobsub[(z,r)]=[]\n",
    "                roi_bck[z]=[]\n",
    "                for t in range(len(self.zTchunk[z].data)):            \n",
    "                    roi_dFF[(z,r),t] = [np.divide(roi_nob[(z,r),t],np.mean(roi_nob_mean[(z,r)]),where=np.mean(roi_nob_mean[(z,r)])!=0)]\n",
    "                    roin_dFF[(z,r)].extend(roi_dFF[(z,r),t])\n",
    "                    roi_dFF_nobsub[(z,r),t] = [np.divide(np.mean(self.roi_pixels[z,t][r]),roi_mean_nobsub[(z,r)],where=roi_mean_nobsub[(z,r)]!=0)]\n",
    "                    roin_dFF_nobsub[(z,r)].extend(roi_dFF_nobsub[(z,r),t])\n",
    "                    roi_bck[z].extend([bckg[z,t]])\n",
    "        for z in range(self.numZ):\n",
    "            for r in range(len(self.roixy[z])):        \n",
    "                rn_dFF[(z,r)] = roin_dFF[(z,r)][:len(roi_bck[z])-1] #there must be at least 1 roi on the last z plane analyzed\n",
    "                rn_dFF_nobsub[(z,r)] = roin_dFF_nobsub[(z,r)][:len(roi_bck[z])-1]\n",
    "        rn_dFF_df=pd.DataFrame.from_dict(rn_dFF)\n",
    "        rn_dFF_nobsub_df=pd.DataFrame.from_dict(rn_dFF_nobsub)\n",
    "        rn_dFF_df.to_pickle(self.path+\"/Quant/rn_dFF.pkl\")\n",
    "        rn_dFF_nobsub_df.to_pickle(self.path+\"/Quant/rn_dFF_nobsub.pkl\")\n",
    "        self.roin_dFF = roin_dFF\n",
    "        self.roin_dFF_nobsub = roin_dFF_nobsub\n",
    "        return rn_dFF, roin_dFF \n",
    "\n",
    "    def roiplots(self):\n",
    "        \"\"\"\n",
    "        Save dFF timetrace of each roi as pdf and pkl\n",
    "        \"\"\"\n",
    "        os.mkdir(self.path+\"/Quant/roiplots\")\n",
    "        for z in range(self.numZ):\n",
    "            for r in range(len(self.roixy[z])):\n",
    "                f=plt.figure(1,figsize=(15,3),facecolor='white')\n",
    "                ax1=plt.subplot(111)\n",
    "                plt.plot(self.roin_dFF[z,r],linewidth=0.3)\n",
    "                fr,labels=plt.xticks()\n",
    "                plt.xticks(fr,(fr*self.exptime*self.numZ).astype(int))\n",
    "                plt.xlabel(\"time (s)\")\n",
    "                plt.ylabel(\"dF/F\")\n",
    "                roin = self.wormID +\"_z\"+str(z)+\"r\"+str(r)\n",
    "                plt.title(roin)\n",
    "                plt.tight_layout()\n",
    "                f.savefig(self.path +'/Quant/roiplots/' + \"dFF_\" + roin + \".pdf\",dpi=1200,format='pdf')\n",
    "                plt.clf()\n",
    "                rn_df=pd.DataFrame.from_dict(self.roin_dFF[z,r])\n",
    "                rn_df.to_pickle(self.path+ \"/Quant/roiplots/\"+ \"dFF_\" + roin+\".pkl\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### use the functions defined above to process the movies recorded and get dFF timeseries for each ROI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = Timeseries()\n",
    "rois = x.ZorganizedROIs()\n",
    "zt  = x.ZorganizedTimepoints()\n",
    "rpixels = x.roipixels()\n",
    "dff = x.dFF()\n",
    "rplots = x.roiplots()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# risetimes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### plot rise times manually for each trace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from matplotlib.widgets import Cursor\n",
    "%matplotlib qt\n",
    "path    = x.path #For saving the rise time files \n",
    "exptime = x.exptime\n",
    "numZ    = x.numZ\n",
    "wormID = x.wormID \n",
    "rn_dFF = dff[0]\n",
    "\"\"\"\n",
    "on every rise, note the risetime (90%-10% of baseline value)\n",
    "\"\"\"\n",
    "z,n,val,addrtime,rtimebegins,rtimeends=({} for i in range(6))\n",
    "for n, p in enumerate(rn_dFF.keys()):\n",
    "    fig,ax = plt.subplots(1,1,num=n,squeeze=True,figsize=(8,6))\n",
    "    ax.plot(rn_dFF[p],linewidth=0.3,marker='.',markersize=1)\n",
    "    ax.set_title(p)\n",
    "    ax.text(0.8, 0.8, numZ*exptime, transform=ax.transAxes, fontsize=8, verticalalignment='top',horizontalalignment='right')              \n",
    "    ax.set_ylabel(\"dF/F\")\n",
    "    cursor = Cursor(ax, useblit=True, color='k', linewidth=1)\n",
    "    zoom_ok = False\n",
    "    print('\\nZoom or pan to view, \\npress spacebar when ready to click:\\n')\n",
    "    while not zoom_ok:\n",
    "        zoom_ok=plt.waitforbuttonpress()\n",
    "    print('Click once to select timepoint:')\n",
    "    val=plt.ginput(n=-1,timeout=0,show_clicks=True,mouse_add=1,mouse_pop=2,mouse_stop=3)\n",
    "    addrtime,rtimebegins,rtimeends=([] for i in range (3))\n",
    "    for num in range(0,len(val),2):\n",
    "        ax.plot(val[num][0],val[num][1],'m.')\n",
    "        ax.plot(val[num+1][0],val[num+1][1],'y.')\n",
    "        ylim=ax.get_ylim()\n",
    "        ax.text(val[num][0],ylim[0]+0.05,np.round(((val[num+1][0]-val[num][0])*(numZ*exptime)),decimals=2),fontsize=8)\n",
    "        rtimebegins.append(val[num][0])\n",
    "        rtimeends.append(val[num+1][0])\n",
    "        addrtime.append(np.round(((val[num+1][0]-val[num][0])*(numZ*exptime)),decimals=2))\n",
    "    xlim=ax.get_xlim()\n",
    "    ax.set_xticks(range(np.int(xlim[0]),np.int(xlim[1]),np.int(20/(exptime*numZ))))\n",
    "    xtl=range(np.int(xlim[0]*exptime*numZ),np.int(xlim[1]*exptime*numZ),20)\n",
    "    ax.set_xticklabels(xtl)#, fontsize=8\n",
    "    ax.axes.tick_params(axis='y')#, labelsize=8\n",
    "    ax.set_xlabel(\"time (s)\")#, fontsize=8\n",
    "#     z[p]=roin_dFF[p].split('r')[0]\n",
    "#     z[p]=np.int(z[p].split('z')[1])\n",
    "#     n[p]=np.int(roin_dFF[p].split('r')[1])\n",
    "#     ax[1].imshow(zT0[z[p]])\n",
    "#     for pix in range(len(roixy[z[p]][n[p]])):\n",
    "#         ax[1].scatter(roixy[z[p]][n[p]][pix][0],roixy[z[p]][n[p]][pix][1],marker='.')            \n",
    "    fig.savefig(path+str(p[0])+str(p[1])+\".pdf\",dpi=1200,format='pdf')\n",
    "    plt.close()\n",
    "    plt.clf()\n",
    "    addrtimesv=pd.DataFrame.from_dict([rtimebegins,rtimeends,addrtime])\n",
    "    addrtimesv.to_pickle(path+\"\\\\\" + x.wormID +\"_z\"+str(p[0])+\"r\"+str(p[1])+\".pkl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### compile dFF timeseries from select ROIs from each worm in one datastructure for that worm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import glob\n",
    "\n",
    "path       = r\"C:\\Users\\heeun\\Documents\\Garrison_lab\\somatag_2019\\20190925w2_nls\\Quant\\roiplots\" ## Path with individual ROI plots in .pdf and .pkl files\n",
    "numZ       = x.numZ #number of z positions; 1 if singleplane\n",
    "exp_time   = x.exptime # exposure time \n",
    " \n",
    "    ## Go through each file in the directory to get the ROI data\n",
    "\n",
    "all_files = glob.glob(path + \"/*.pkl\")\n",
    "ROI_list      = [] \n",
    " \n",
    "analysis_path = path + \"/analysis\" \n",
    "if not os.path.exists(analysis_path):\n",
    "    os.makedirs(analysis_path) \n",
    "       \n",
    "for i, file_name in enumerate (all_files):\n",
    "    file_name_cut = file_name[(len(path)+1):-4]\n",
    "    ROIid         = file_name_cut.split(\"_\")[-1]\n",
    "    ROI_list.append(ROIid)\n",
    "    temp          = pd.read_pickle(file_name)\n",
    "    if i == 0:\n",
    "        wormID        = file_name_cut.split(\"_z\")[0]\n",
    "        total_frames  = len(temp)\n",
    "        df_dFF        = pd.DataFrame(np.arange(0, total_frames*exp_time*numZ, exp_time*numZ), columns = ['time_sec']) \n",
    "    df_dFF[file_name_cut] = temp\n",
    "    \n",
    "print (ROI_list)\n",
    "    \n",
    "save_file_path  = analysis_path + \"\\\\\" + wormID\n",
    "df_dFF.to_pickle(save_file_path + \".pkl\")\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### compile all dFF timeseries in one datastructure automated risetime calculation by peakfinding\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "import pickle as pk\n",
    "\n",
    "\n",
    "path = r\"C:\\Users\\heeun\\Documents\\Garrison_lab\\somatag_2019\\all_trace_pkls\" \n",
    " # directory with .pkl files from all worms with all selected ROIs\n",
    "all_files = glob.glob(path + \"/*.pkl\")\n",
    "analysis_path = path + \"/analysis\" \n",
    "if not os.path.exists(analysis_path):\n",
    "    os.makedirs(analysis_path) \n",
    "\n",
    "wormID_list   = [] \n",
    "dict_all_ROIs = {}\n",
    "ROI_list      = {}\n",
    "\n",
    "for i, file_path in enumerate (all_files):\n",
    "    wormID           = file_path[(len(path)+5):-4]\n",
    "    wormID_list.append(wormID) \n",
    "    temp             = pd.read_pickle(file_path)\n",
    "    dFF_all          = temp\n",
    "    time_sec         = dFF_all[\"time_sec\"]\n",
    "    dFF_all          = dFF_all.drop(columns=['time_sec']) #OK\n",
    "    ROI_list[wormID] = list(dFF_all.columns)\n",
    "     \n",
    "    for j, ROIid in enumerate(ROI_list[wormID]):\n",
    "        dict_all_ROIs[ROIid]              = {}        \n",
    "\n",
    "\n",
    "for i, file_path in enumerate (all_files):\n",
    "    temp             = pd.read_pickle(file_path)\n",
    "    df_dFF_all       = temp\n",
    "    time_sec         = df_dFF_all[\"time_sec\"]\n",
    "    df_dFF_all       = df_dFF_all.drop(columns=[\"time_sec\"]) \n",
    "\n",
    "    for j, ROIid in enumerate(ROI_list[wormID]):\n",
    "        dict_all_ROIs[ROIid][\"WormID\"]    = wormID\n",
    "        dict_all_ROIs[ROIid][\"GCaMPtype\"] = wormID[-3:]\n",
    "        dict_all_ROIs[ROIid][\"Zplane\"]    = ROIid.split(\"_z\")[1].split(\"r\")[0]\n",
    "        dict_all_ROIs[ROIid][\"ROInumber\"] = ROIid.split(\"_z\")[1].split(\"r\")[1]\n",
    "        dict_all_ROIs[ROIid][\"dFF\"]       = dFF_all[ROIid]\n",
    "        dict_all_ROIs[ROIid][\"time_sec\"]  = time_sec      \n",
    "save_file_path  = analysis_path+ \"/allROIs.pkl\" \n",
    "\n",
    "output = open(save_file_path, 'wb')\n",
    "pk.dump(dict_all_ROIs, output)\n",
    "output.close()    \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### automated risetime calculation by peakfinding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "def peakfind(vec, thresh=1, relthreshflag = True, peaksearchingmode=1):\n",
    "# find the peaks and valleys of a time series using threshold tracking\n",
    "#\n",
    "#    [PEAKS,VALLEYS] = peakfind(VEC,THRESH,PEAKSEARCHMODESTART) \n",
    "#\n",
    "#    input:\n",
    "#    VEC is a vector time series\n",
    "#    THRESH is an absolute threshold criterion (scalar)\n",
    "#\n",
    "#    output:\n",
    "#    PEAKS and VALLEYS are lists ofindices\n",
    "#\n",
    "#    if peaksearchingmode=1 then first search for a peak as we\n",
    "#    scan from left to right\n",
    "#    else if peaksearchingmode ~= 1 otherwise first search for a valley\n",
    "#    \n",
    "#    by default, search for a peak first (say, if initial signal deflection \n",
    "#    is positive)\n",
    "#\n",
    "#   hints:\n",
    "#    len(PEAKS) gives number of peaks\n",
    "#    len(PEAKS) and len(VALLEYS) differ by at most 1\n",
    "#    peaks and valleys always alternate    \n",
    "#\n",
    "#    saul.kato@ucsf.edu\n",
    "#\n",
    "\n",
    "  if relthreshflag:\n",
    "    thresh = thresh * (np.max(vec) - np.min(vec))\n",
    "\n",
    "\n",
    "  peaks = []\n",
    "  valleys = []\n",
    "  max_tracker_val = -np.inf\n",
    "  min_tracker_val = np.inf\n",
    "  max_tracker_index = np.nan\n",
    "  min_tracker_index = np.nan\n",
    "\n",
    "  for i in range(len(vec)):\n",
    "      \n",
    "    vi=vec[i]\n",
    "    \n",
    "    # update max_tracker_val if needed\n",
    "    if vi > max_tracker_val:\n",
    "      max_tracker_val=vi\n",
    "      max_tracker_index=i\n",
    "\n",
    "    # update min_tracker_val if needed\n",
    "    if vi < min_tracker_val:\n",
    "      min_tracker_val=vi\n",
    "      min_tracker_index=i\n",
    "    \n",
    "    if peaksearchingmode==1:\n",
    "      if vi < (max_tracker_val - thresh):\n",
    "        peaks.append(max_tracker_index) # add entry to peaks\n",
    "        min_tracker_index=i  # move up min tracker to current time\n",
    "        min_tracker_val=vi\n",
    "        peaksearchingmode=0  # switch to valley searching\n",
    "    else:\n",
    "      if vi > (min_tracker_val + thresh):\n",
    "        valleys.append(min_tracker_index) # add entry to valleys\n",
    "        max_tracker_index=i  # move up max tracker to current time\n",
    "        max_tracker_val=vi\n",
    "        peaksearchingmode=1  # switch to peak searching\n",
    "    \n",
    "  return peaks, valleys\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "define function to find rise transients, using peakfinding method\n",
    "\"\"\"\n",
    "\n",
    "def risetimes(timeseries, sm=20, thresh=5):\n",
    " \n",
    "    #3-value box filter\n",
    "    xs=timeseries\n",
    "    # find peaks\n",
    "    peaks, _ = peakfind(xs,thresh=thresh,relthreshflag=True)\n",
    "    rise_start_frames=np.zeros(len(peaks))\n",
    "    numframes=np.zeros(len(peaks))\n",
    "    \n",
    "    # derivative of smoothed timeseries\n",
    "    xd=np.diff(timeseries.rolling(sm).mean())\n",
    "    \n",
    "    # count backwards from peaks\n",
    "    for p in range(len(peaks)):\n",
    "        j=peaks[p]-1\n",
    "        num=0\n",
    "        while np.nan_to_num(xd[j]) > 0 and j > 0:\n",
    "            num=num+1\n",
    "            j=j-1\n",
    "        numframes[p]=num\n",
    "    \n",
    "    rise_end_frames = peaks\n",
    "    rise_start_frames = np.subtract(peaks,numframes)\n",
    "    rise_start_frames = rise_start_frames.astype(int)  #hack\n",
    "    return rise_start_frames, rise_end_frames\n",
    "\"\"\"\n",
    "measure all rise transients\n",
    "\"\"\"\n",
    "threshold=.4\n",
    "smoothing=40\n",
    "for d in data:\n",
    "    data[d]['rise_start_frames'],data[d]['rise_end_frames'] = risetimes(data[d]['dFF'],thresh=threshold,sm=smoothing)\n",
    "    data[d]['risetimes']=np.zeros(len(data[d]['rise_start_frames']))\n",
    "    for t in range(len(data[d]['rise_start_frames'])):\n",
    "        data[d]['risetimes'][t]=data[d]['time_sec'][data[d]['rise_end_frames'][t]] - data[d]['time_sec'][data[d]['rise_start_frames'][t]]\n",
    "\"\"\"\n",
    "save to .pkl\n",
    "\"\"\"\n",
    "output = open(r'C:\\Users\\heeun\\Documents\\Garrison_lab\\somatag_2019\\all_trace_pkls\\analysis\\allROIs_output.pkl', 'wb')\n",
    "pickle.dump(data, output)\n",
    "output.close()\n",
    "\"\"\"\n",
    "plot traces with detected transients overlaid\n",
    "\"\"\"\n",
    "num_traces=len(data)\n",
    "figsize = (20, 2*num_traces)\n",
    "fig = plt.figure(figsize=figsize)\n",
    "for d,t in zip(data,range(num_traces)):\n",
    "    ax = plt.subplot(num_traces,1,t+1)\n",
    "    ax.plot(data[d]['time_sec'],data[d]['dFF'])\n",
    "    for r in range(len(data[d]['risetimes'])):\n",
    "        r1=data[d]['rise_start_frames'][r]\n",
    "        r2=data[d]['rise_end_frames'][r]\n",
    "        ax.plot(data[d]['time_sec'][r1:r2],data[d]['dFF'][r1:r2],'r')\n",
    "        ax.text(data[d]['time_sec'][r1],0,\"{:.2f}\".format(data[d]['risetimes'][r]))\n",
    "    ax.set_xlabel('time (s)')\n",
    "    ax.set_ylabel('DF/F')\n",
    "    ax.text(.5,.9,d,\n",
    "    horizontalalignment='center',\n",
    "    fontsize=10,\n",
    "    transform=ax.transAxes)\n",
    "\n",
    "fig.savefig(r\"C:\\Users\\heeun\\Documents\\Garrison_lab\\somatag_2019\\all_trace_pkls\\analysis\\traces_with_risetimes-sm\"+ str(smoothing)+ \"-tr\"+ str(threshold) +\".pdf\", bbox_inches='tight')\n",
    "\n",
    "\"\"\"\n",
    "histograms of risetimes, throwing out spurious fast events\n",
    "\"\"\"\n",
    "rib_risetimes=[]\n",
    "nls_risetimes=[]\n",
    "\n",
    "min_thresh=1.5\n",
    "\n",
    "for d in data:\n",
    "    if data[d]['GCaMPtype']=='rib':\n",
    "        rib_risetimes.extend(list(filter(lambda x: (x > min_thresh),list(data[d]['risetimes']))))\n",
    "    else:\n",
    "        nls_risetimes.extend(list(filter(lambda x: (x > min_thresh),list(data[d]['risetimes']))))\n",
    "\n",
    "figsize = (4, 4)\n",
    "fig = plt.figure(figsize=figsize)\n",
    "\n",
    "_, bins, _ = plt.hist(rib_risetimes,range=[0, 30],bins=80,label='ribo')\n",
    "_ = plt.hist(nls_risetimes,range=[0, 30],bins=bins,label='nls',alpha=0.5)\n",
    "plt.legend(loc='upper right')\n",
    "plt.xlabel(\"rise time (s)\")\n",
    "plt.show()\n",
    "fig.savefig(r'C:\\Users\\heeun\\Documents\\Garrison_lab\\somatag_2019\\all_trace_pkls\\analysis\\histogram_all_ribo_vs_nls' + str(smoothing)+ \"-tr\"+ str(threshold) + '.pdf', bbox_inches='tight')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modeling the temporal transformation between ribo and nls"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### create synthetic ribo-gcamp ramp trace generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "time_vec=np.arange(-40,40,0.1)\n",
    "\n",
    "def gen_ribo_trace(rise_time,in_time_vec):\n",
    "    out_vec=np.zeros(len(time_vec))\n",
    "    for i in range(len(time_vec)):\n",
    "        if in_time_vec[i]>0:\n",
    "            if in_time_vec[i]<rise_time:\n",
    "                out_vec[i]=in_time_vec[i]/rise_time\n",
    "            else:\n",
    "                out_vec[i]=1.0\n",
    "                                   \n",
    "    return out_vec \n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "plt.plot(time_vec,gen_ribo_trace(4.0,time_vec))\n",
    "plt.xlabel(\"time (s)\")\n",
    "plt.ylabel(\"magnitude\")\n",
    "ax.axvline(x=0, color='gray')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### create first-order impulse response function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gen_irf(tau,in_time_vec):\n",
    "    out_vec=np.zeros(len(in_time_vec))\n",
    "    for i in range(len(in_time_vec)):\n",
    "        if in_time_vec[i]>0:\n",
    "                out_vec[i]=np.exp(-in_time_vec[i]/tau)\n",
    "    return out_vec\n",
    "\n",
    "time_vec=np.arange(-40,40,0.1)\n",
    "fig, ax = plt.subplots()\n",
    "plt.plot(time_vec,gen_irf(9.0,time_vec))\n",
    "plt.xlabel(\"time (s)\")\n",
    "plt.ylabel(\"magnitude\")\n",
    "ax.axvline(x=0, color='gray')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### create nls trace generator as convolution of ribo-gcamp traces and irf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gen_nls_trace(rise_time,tau,in_time_vec):   \n",
    "    convolved_vec=np.convolve(gen_ribo_trace(rise_time,in_time_vec),gen_irf(tau,in_time_vec),mode='same') \n",
    "    out_vec=convolved_vec\n",
    "    return out_vec\n",
    "\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "plt.plot(time_vec,gen_nls_trace(4.0,9.0,time_vec))\n",
    "plt.xlabel(\"time (s)\")\n",
    "plt.ylabel(\"magnitude\")\n",
    "ax.axvline(x=0, color='gray')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### measure rise time of trace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def measure_rise_time(trace,in_time_vec,threshold_low=0.05,threshold_high=0.95):\n",
    "    \n",
    "    # find start time\n",
    "    start_time=in_time_vec[np.argmax(trace > (max(trace)-min(trace))*threshold_low+min(trace))]\n",
    "    \n",
    "    # find end time\n",
    "    end_time=in_time_vec[np.argmax(trace > (max(trace)-min(trace))*threshold_high+min(trace))]\n",
    "\n",
    "    rise_time = end_time - start_time\n",
    "    return rise_time, start_time, end_time\n",
    " \n",
    "    \n",
    "test_trace=gen_nls_trace(4.0,6.0,time_vec)       \n",
    "test_rise_time, test_start_time, test_end_time = measure_rise_time(test_trace,time_vec)\n",
    "    \n",
    "fig, ax = plt.subplots()\n",
    "plt.plot(time_vec,gen_nls_trace(4.0,6.0,time_vec))\n",
    "plt.text(test_start_time, 0, 'risetime={:.2f}'.format(test_rise_time))\n",
    "plt.xlabel(\"time (s)\")\n",
    "plt.ylabel(\"magnitude\")\n",
    "ax.axvline(x=0, color='gray')\n",
    "ax.axvline(x=test_start_time, color='red')\n",
    "ax.axvline(x=test_end_time, color='red')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### transform ribo-gcamp rise times into nls-gcamp rise times given an i.r.f. tau"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform_rise_times(ribo_vec,tau):\n",
    "    \n",
    "    this_time_vec=np.arange(-40,40,0.1)\n",
    "    nls_vec=[]\n",
    "    \n",
    "    for time in ribo_vec:\n",
    "        rt, _, _ = measure_rise_time(gen_nls_trace(time,tau,this_time_vec),this_time_vec,\n",
    "                                     threshold_low=0.05,threshold_high=0.99)\n",
    "        nls_vec = np.append(nls_vec,rt)\n",
    "    \n",
    "    return nls_vec"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### create synthetic test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_ribo=abs((np.random.randn(200)*1.3)+4)\n",
    "\n",
    "test_nls=transform_rise_times(test_ribo,tau=3.1)\n",
    "\n",
    "_, bins, _ = plt.hist(test_ribo,range=[0, 30],bins=50,label='ribo')\n",
    "_ = plt.hist(test_nls,range=[0, 30],bins=bins,label='nls',alpha=0.5)\n",
    "plt.legend(loc='upper right')\n",
    "plt.xlabel(\"rise time (s)\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(test_ribo,test_nls,'.')\n",
    "plt.xlabel(\"ribo rise time (s)\")\n",
    "plt.ylabel(\"nls rise time (s)\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### define histogram-comparison error function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def error_func(tau,vec1,vec2,num_bins=30,range=[0, 20]):\n",
    "    \n",
    "    # make hist\n",
    "    hist_exper=np.histogram(vec2,bins=num_bins,range=range)\n",
    "    \n",
    "    # transform rise_times\n",
    "    vec2_model = transform_rise_times(vec1,tau)\n",
    "    hist_model=np.histogram(vec2_model,bins=num_bins,range=range)\n",
    "    \n",
    "    # compare hists\n",
    "    err=distance.euclidean(hist_exper[0],hist_model[0])\n",
    "    return err "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### find tau that best explains data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res=optimize.minimize(error_func,2.5,args=(test_ribo, test_nls),method='nelder-mead',options={'disp': True})\n",
    "res.x[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### plot tau error landscape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_tau_vec = np.arange(1,5,0.05)\n",
    "my_error_vec = []\n",
    "for my_tau in my_tau_vec:\n",
    "    my_error_vec=np.append(my_error_vec,error_func(my_tau,test_ribo,test_nls))\n",
    "    \n",
    "plt.plot(my_tau_vec,my_error_vec)    \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Now run this model on experimental data- comparing ribo and nls aggregate data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### create time series excerpts of ribo rise transients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rib_excerpts_dFF=[] \n",
    "rib_excerpts_time_sec=[]\n",
    "for d in data:\n",
    "    if data[d]['GCaMPtype']=='rib':\n",
    "        for r in range(len(data[d]['rise_start_frames'])):\n",
    "            rib_excerpts_dFF.append(data[d]['dFF'][data[d]['rise_start_frames'][r]:(50+data[d]['rise_end_frames'][r])])\n",
    "            rib_excerpts_time_sec.append(data[d]['time_sec'][data[d]['rise_start_frames'][r]:(50+data[d]['rise_end_frames'][r])])\n",
    "\n",
    "for x in rib_excerpts_dFF:\n",
    "    plt.plot(x.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "risetimes_rib=[]\n",
    "for r,t in zip(rib_excerpts_dFF,rib_excerpts_time_sec):\n",
    "    if not r.empty:\n",
    "        rt,_,_ = measure_rise_time(r.values,t.values,threshold_low=0.05,threshold_high=0.95)\n",
    "        risetimes_rib.append(rt)\n",
    "risetimes_rib=list(filter(lambda x: (x > min_thresh),risetimes_rib))\n",
    "plt.hist(risetimes_rib,bins=30)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### function to convolve 1st order filter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gen_irf(tau,in_time_vec):\n",
    "    out_vec=np.zeros(len(in_time_vec))\n",
    "    for i in range(len(in_time_vec)):\n",
    "        if in_time_vec[i]>0:\n",
    "                out_vec[i]=np.exp(-in_time_vec[i]/tau)\n",
    "    return out_vec\n",
    "\n",
    "\n",
    "def convolve_ribo_trace(ribo_trace,tau,in_time_vec):   \n",
    "    convolved_vec=np.convolve(ribo_trace,gen_irf(tau,in_time_vec),mode='same') \n",
    "    return convolved_vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tau=1\n",
    "for r,t in zip(rib_excerpts_dFF,rib_excerpts_time_sec):\n",
    "    if not r.empty:\n",
    "        plt.plot(convolve_ribo_trace(r.values,tau,t.values))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_nls_traces(tau=10):\n",
    "    risetimes_sim=[]\n",
    "    for r,t in zip(rib_excerpts_dFF,rib_excerpts_time_sec):\n",
    "        if not r.empty:\n",
    "            nls_sim_trace=convolve_ribo_trace(r.values,tau,t.values)\n",
    "            rt,_,_ = measure_rise_time(nls_sim_trace,t.values,threshold_low=0.05,threshold_high=0.95)\n",
    "            risetimes_sim.append(rt)\n",
    "        risetimes_sim=list(filter(lambda x: (x > min_thresh),risetimes_sim))     \n",
    "    return risetimes_sim  \n",
    "\n",
    "\n",
    "plt.hist(make_nls_traces(7),bins=30)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### create time series excerpts of nls rise transients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nls_excerpts_dFF=[] \n",
    "nls_excerpts_time_sec=[]\n",
    "for d in data:\n",
    "    if data[d]['GCaMPtype']=='nls':\n",
    "        for r in range(len(data[d]['rise_start_frames'])):\n",
    "            nls_excerpts_dFF.append(data[d]['dFF'][data[d]['rise_start_frames'][r]:data[d]['rise_end_frames'][r]])\n",
    "            nls_excerpts_time_sec.append(data[d]['time_sec'][data[d]['rise_start_frames'][r]:data[d]['rise_end_frames'][r]])\n",
    "\n",
    "for x in nls_excerpts_dFF:\n",
    "    plt.plot(x.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "risetimes_nls_actual=[]\n",
    "\n",
    "for r,t in zip(nls_excerpts_dFF,nls_excerpts_time_sec):\n",
    "    if not r.empty:\n",
    "        rt,_,_ = measure_rise_time(r.values,t.values,threshold_low=0.05,threshold_high=0.95)\n",
    "        risetimes_nls_actual.append(rt)\n",
    "\n",
    "risetimes_nls_actual=list(filter(lambda x: (x > min_thresh),risetimes_nls_actual))   \n",
    "plt.hist(risetimes_nls_actual,bins=30)\n",
    "plt.show"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def hist_error_func(tau):\n",
    "\n",
    "    num_bins=30\n",
    "    range=[0, 10]\n",
    "    \n",
    "    # make hists\n",
    "    hist_exper=np.histogram(make_nls_traces(tau),bins=num_bins,range=range)\n",
    "    hist_model=np.histogram(risetimes_nls_actual,bins=num_bins,range=range)\n",
    "    \n",
    "    # compare hists\n",
    "    err=distance.euclidean(hist_exper[0],hist_model[0])\n",
    "    return err "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res=optimize.minimize(hist_error_func,3,method='nelder-mead',options={'disp': True})\n",
    "res.x[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_tau_vec = np.arange(1,8,0.05)\n",
    "my_error_vec = []\n",
    "for my_tau in my_tau_vec:\n",
    "    my_error_vec=np.append(my_error_vec,hist_error_func(my_tau))\n",
    "    \n",
    "plt.plot(my_tau_vec,my_error_vec)    \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res=optimize.minimize(error_func,2.5,args=(test_ribo, test_nls),method='nelder-mead',options={'disp': True})\n",
    "res.x[0]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
